{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ff17ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T09:43:47.888136Z",
     "iopub.status.busy": "2024-11-29T09:43:47.887754Z",
     "iopub.status.idle": "2024-11-29T09:48:41.626259Z",
     "shell.execute_reply": "2024-11-29T09:48:41.625012Z"
    },
    "papermill": {
     "duration": 293.74939,
     "end_time": "2024-11-29T09:48:41.628321",
     "exception": false,
     "start_time": "2024-11-29T09:43:47.878931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n",
      "Processing /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\r\n",
      "Installing collected packages: logits-processor-zoo\r\n",
      "Successfully installed logits-processor-zoo-0.1.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "logits-processor-zoo 0.1.0 requires accelerate<0.27.0,>=0.26.1, but you have accelerate 0.34.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mCPU times: user 4.21 s, sys: 943 ms, total: 5.15 s\n",
      "Wall time: 4min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vllm\n",
    "!pip uninstall -y torch\n",
    "!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl\n",
    "!pip install --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\n",
    "\n",
    "# Peft\n",
    "!pip install transformers peft accelerate \\\n",
    "    -q -U --no-index --find-links /kaggle/input/lmsys-wheel-files\n",
    "!pip install -q --no-index /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "!pip install -q --no-index  /kaggle/input/bitsandbytes0-42-0/optimum-1.21.2-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "!pip install -q --no-index  /kaggle/input/bitsandbytes0-42-0/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --find-links=/kaggle/input/bitsandbytes0-42-0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c20088c",
   "metadata": {
    "papermill": {
     "duration": 0.003554,
     "end_time": "2024-11-29T09:48:41.635950",
     "exception": false,
     "start_time": "2024-11-29T09:48:41.632396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LLM Reasoning\n",
    "Prompt LLM to identify likely misconception that led to wrong answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "644469d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T09:48:41.646795Z",
     "iopub.status.busy": "2024-11-29T09:48:41.646424Z",
     "iopub.status.idle": "2024-11-29T09:48:42.577735Z",
     "shell.execute_reply": "2024-11-29T09:48:42.576995Z"
    },
    "papermill": {
     "duration": 0.938519,
     "end_time": "2024-11-29T09:48:42.579793",
     "exception": false,
     "start_time": "2024-11-29T09:48:41.641274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "explode_df = []\n",
    "for idx, row in df.iterrows():\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        if option == row[\"CorrectAnswer\"]:\n",
    "            continue\n",
    "        correct_option = row[\"CorrectAnswer\"]\n",
    "\n",
    "        explode_df.append({\"QuestionId_Answer\": f\"{row.QuestionId}_{option}\",\n",
    "                           \"ConstructName\": row.ConstructName,\n",
    "                           \"SubjectName\": row.SubjectName,\n",
    "                           \"QuestionText\": row.QuestionText,\n",
    "                           \"CorrectAnswer\": row[f\"Answer{correct_option}Text\"],\n",
    "                           \"IncorrectAnswer\": row[f\"Answer{option}Text\"]\n",
    "                           })\n",
    "\n",
    "df = pd.DataFrame(explode_df)\n",
    "df.to_csv(\"explode_df.csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1984776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T09:48:42.588805Z",
     "iopub.status.busy": "2024-11-29T09:48:42.588542Z",
     "iopub.status.idle": "2024-11-29T09:48:42.595377Z",
     "shell.execute_reply": "2024-11-29T09:48:42.594475Z"
    },
    "papermill": {
     "duration": 0.013317,
     "end_time": "2024-11-29T09:48:42.597071",
     "exception": false,
     "start_time": "2024-11-29T09:48:42.583754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vllm_reasoning.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vllm_reasoning.py\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subjectName = self.data.loc[idx, \"SubjectName\"]\n",
    "        constructName = self.data.loc[idx, \"ConstructName\"]\n",
    "        question = self.data.loc[idx, \"QuestionText\"]\n",
    "        correct_answer = self.data.loc[idx, \"CorrectAnswer\"]\n",
    "        wrong_answer = self.data.loc[idx, \"IncorrectAnswer\"]\n",
    "        \n",
    "\n",
    "        prompt = f\"\"\"Here is a question about {constructName} ({subjectName}):\n",
    "        \n",
    "- Question: {question}\n",
    "- Correct Answer: {correct_answer}\n",
    "- Wrong Answer: {wrong_answer}\n",
    "        \n",
    "Please provide a detailed analysis on what misconception or reasoning error that cause the student to derive the wrong answer. Focus only on explaining the misconception.\n",
    "\"\"\"\n",
    "    \n",
    "        message = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a proficient Mathematics teacher. Your goal is to identify the likely misconception or reasoning error that led the student to choose the wrong answer.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt.strip()}\n",
    "        ]\n",
    "        \n",
    "        return message, correct_answer, str(self.data.loc[idx, \"QuestionId_Answer\"])\n",
    "    \n",
    "def collate_batch(batch):\n",
    "    data, labels, question_ids = zip(*batch)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        data,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text, labels, question_ids\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"explode_df.csv\")\n",
    "    model_name = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "    device = \"cuda\" # the device to load the model onto\n",
    "    model = LLM(model_name,\n",
    "                quantization=\"awq\",\n",
    "                gpu_memory_utilization=1,\n",
    "                tensor_parallel_size=2,\n",
    "                trust_remote_code=True,\n",
    "                dtype=\"half\",\n",
    "                max_model_len=4000)\n",
    "    \n",
    "    tokenizer = model.get_tokenizer()\n",
    "    train_dataset = MathDataset(df)\n",
    "    train_pbar = tqdm(DataLoader(train_dataset, batch_size=32, collate_fn=collate_batch))\n",
    "    \n",
    "    llm_response = []\n",
    "    for model_inputs, answers, question_ids in train_pbar:\n",
    "        outputs = model.generate(\n",
    "            model_inputs,\n",
    "            SamplingParams(\n",
    "                n=1,\n",
    "                temperature=0,\n",
    "                seed=111,\n",
    "                max_tokens=1024\n",
    "            ),\n",
    "            use_tqdm=False\n",
    "        )\n",
    "        \n",
    "        for i in range(len(outputs)):\n",
    "            output = outputs[i]\n",
    "            llm_response.append({\"QuestionId_Answer\": question_ids[i], \"Misconception\": output.outputs[0].text})\n",
    "            \n",
    "    llm_misconception = pd.DataFrame(llm_response)\n",
    "    llm_misconception.to_csv(\"llm_misconception.csv\")\n",
    "    #llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5968425b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T09:48:42.605429Z",
     "iopub.status.busy": "2024-11-29T09:48:42.605205Z",
     "iopub.status.idle": "2024-11-29T09:53:34.406755Z",
     "shell.execute_reply": "2024-11-29T09:53:34.405530Z"
    },
    "papermill": {
     "duration": 291.808496,
     "end_time": "2024-11-29T09:53:34.409311",
     "exception": false,
     "start_time": "2024-11-29T09:48:42.600815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-29 09:48:47 config.py:246] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 11-29 09:48:47 config.py:715] Defaulting to use mp for distributed inference\r\n",
      "INFO 11-29 09:48:47 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, use_v2_block_manager=False, enable_prefix_caching=False)\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "INFO 11-29 09:48:47 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "INFO 11-29 09:48:47 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:48:47 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:48:47 selector.py:54] Using XFormers backend.\r\n",
      "INFO 11-29 09:48:47 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:48:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:48:50 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "INFO 11-29 09:48:50 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "INFO 11-29 09:48:50 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:48:50 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 11-29 09:48:51 custom_all_reduce_utils.py:202] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 11-29 09:48:58 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:48:58 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "WARNING 11-29 09:48:58 custom_all_reduce.py:127] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m WARNING 11-29 09:48:58 custom_all_reduce.py:127] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 11-29 09:48:58 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7ee72874f790>, local_subscribe_port=37013, local_sync_port=38629, remote_subscribe_port=None, remote_sync_port=None)\r\n",
      "INFO 11-29 09:48:58 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:48:58 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "INFO 11-29 09:48:58 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 11-29 09:48:58 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:48:58 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:48:58 selector.py:54] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:19<01:18, 19.65s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:37<00:56, 18.78s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [01:01<00:41, 20.97s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [01:21<00:20, 20.82s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:01<00:00, 27.44s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:01<00:00, 24.23s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:51:00 model_runner.py:692] Loading model weights took 9.0933 GB\r\n",
      "INFO 11-29 09:51:00 model_runner.py:692] Loading model weights took 9.0933 GB\r\n",
      "INFO 11-29 09:51:07 distributed_gpu_executor.py:56] # GPU blocks: 1613, # CPU blocks: 2048\r\n",
      "INFO 11-29 09:51:11 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n",
      "INFO 11-29 09:51:11 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:51:12 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:51:12 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "INFO 11-29 09:52:18 model_runner.py:1181] Graph capturing finished in 67 secs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 11-29 09:52:18 model_runner.py:1181] Graph capturing finished in 67 secs.\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [01:11<00:00, 71.62s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!python vllm_reasoning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec9254a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T09:53:34.421908Z",
     "iopub.status.busy": "2024-11-29T09:53:34.421529Z",
     "iopub.status.idle": "2024-11-29T09:53:37.373323Z",
     "shell.execute_reply": "2024-11-29T09:53:37.372069Z"
    },
    "papermill": {
     "duration": 2.961092,
     "end_time": "2024-11-29T09:53:37.375864",
     "exception": false,
     "start_time": "2024-11-29T09:53:34.414772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 29 09:53:37 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   73C    P0             33W /   70W |       1MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   73C    P0             31W /   70W |       1MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f371aaf4",
   "metadata": {
    "papermill": {
     "duration": 0.004977,
     "end_time": "2024-11-29T09:53:37.386332",
     "exception": false,
     "start_time": "2024-11-29T09:53:37.381355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compute cosine similarity of embedding\n",
    "Compute embeddings of **MisconceptionName** and LLM reasoned possible misconceptions using [SFR-Embedding-Mistral](https://huggingface.co/Salesforce/SFR-Embedding-Mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9593751d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T09:53:37.399071Z",
     "iopub.status.busy": "2024-11-29T09:53:37.397864Z",
     "iopub.status.idle": "2024-11-29T09:53:37.903590Z",
     "shell.execute_reply": "2024-11-29T09:53:37.902738Z"
    },
    "papermill": {
     "duration": 0.514247,
     "end_time": "2024-11-29T09:53:37.905646",
     "exception": false,
     "start_time": "2024-11-29T09:53:37.391399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f\"Instruct: {task_description}\\nQuery: {query}\"\n",
    "\n",
    "\n",
    "def inference(df, misconception_df):\n",
    "    device = \"cuda\"\n",
    "    task = \"Given the likely misconception or reasoning error that led the student to choose the wrong answer, please retrieve the most accurate description of the misconception.\"\n",
    "    quries = [get_detailed_instruct(task, str(q)) for q in df[\"Misconception\"].values]\n",
    "    passages = [str(mis) for mis in misconception_df[\"MisconceptionName\"].values]\n",
    "   \n",
    "    # load model and tokenizer\n",
    "    lora_path=\"/kaggle/input/v7-recall/epoch_19_model/adapter.bin\"\n",
    "    model_path = \"/kaggle/input/sfr-embedding-mistral/SFR-Embedding-2_R\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_path.replace(\"/adapter.bin\", \"\"))\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16                  \n",
    "            )\n",
    "    \n",
    "    backbone = AutoModel.from_pretrained(model_path, quantization_config=bnb_config, device_map=device)\n",
    "    config = LoraConfig(\n",
    "            r=64,\n",
    "            lora_alpha=128,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "            ],\n",
    "            bias=\"none\",\n",
    "            lora_dropout=0.05,  # Conventional\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "    model = get_peft_model(backbone, config)\n",
    "    d = torch.load(lora_path, map_location=model.device)\n",
    "    model.load_state_dict(d, strict=False)\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    max_length = 4096\n",
    "    input_texts = quries + passages\n",
    "    batch_size = 8\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(text) for text in input_texts])\n",
    "    input_texts_sorted = [input_texts[idx] for idx in length_sorted_idx]\n",
    "    \n",
    "    for start_index in trange(0, len(input_texts), batch_size, desc=\"Batches\", disable=False):\n",
    "        batch_dict = tokenizer(input_texts_sorted[start_index: start_index + batch_size], max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        batch_dict = batch_to_device(batch_dict, device)  \n",
    "        with torch.no_grad():\n",
    "            outputs = model.model(**batch_dict)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "            embeddings = embeddings.detach().cpu()\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "        all_embeddings.extend(embeddings.detach().cpu())\n",
    "\n",
    "    all_embeddings = np.array([all_embeddings[idx] for idx in np.argsort(length_sorted_idx)])\n",
    "    num_quries = len(quries)\n",
    "    scores = (all_embeddings[:num_quries] @ all_embeddings[num_quries:].T) * 100\n",
    "    top_100 = np.argsort(-scores, axis=1)[:, :100] # top 100 similar misconception\n",
    "    return top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b00a4d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T09:53:37.917812Z",
     "iopub.status.busy": "2024-11-29T09:53:37.917507Z",
     "iopub.status.idle": "2024-11-29T10:05:15.391155Z",
     "shell.execute_reply": "2024-11-29T10:05:15.390348Z"
    },
    "papermill": {
     "duration": 697.482173,
     "end_time": "2024-11-29T10:05:15.393417",
     "exception": false,
     "start_time": "2024-11-29T09:53:37.911244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2dfcb3a0494002a1aec06a75a263a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 325/325 [10:18<00:00,  1.90s/it]\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "llm_misconception = pd.read_csv(\"llm_misconception.csv\")\n",
    "df[\"Misconception\"] = llm_misconception[\"Misconception\"]\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "\n",
    "top_100 = inference(df, misconception_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a96c28dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T10:05:15.435063Z",
     "iopub.status.busy": "2024-11-29T10:05:15.434025Z",
     "iopub.status.idle": "2024-11-29T10:05:15.460615Z",
     "shell.execute_reply": "2024-11-29T10:05:15.459696Z"
    },
    "papermill": {
     "duration": 0.049063,
     "end_time": "2024-11-29T10:05:15.462258",
     "exception": false,
     "start_time": "2024-11-29T10:05:15.413195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>CorrectAnswer</th>\n",
       "      <th>IncorrectAnswer</th>\n",
       "      <th>Misconception</th>\n",
       "      <th>top_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>The student's misconception likely stems from ...</td>\n",
       "      <td>315 1345 2488 1392 2586 2306 1054 1005 2532 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>The student's misconception likely stems from ...</td>\n",
       "      <td>2488 315 1345 1084 1392 2586 373 2532 77 969 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "      <td>The student's misconception likely stems from ...</td>\n",
       "      <td>2532 706 77 1392 1507 1672 1005 2586 1345 871 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>The student's misconception likely stems from ...</td>\n",
       "      <td>891 143 59 1540 2398 2078 167 885 2021 1610 36...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>The student's misconception likely stems from ...</td>\n",
       "      <td>143 891 885 2078 2398 1540 59 265 715 1610 159...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>The student's misconception likely stems from ...</td>\n",
       "      <td>891 143 1610 2398 885 59 2078 715 979 1540 126...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Only\\nTom</td>\n",
       "      <td>The student likely chose \"Only Tom\" because th...</td>\n",
       "      <td>1287 1408 2439 2408 1073 1059 1923 1975 227 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Both Tom and Katie</td>\n",
       "      <td>The misconception that led the student to inco...</td>\n",
       "      <td>1287 1073 1408 2408 2439 557 1923 1765 1338 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Neither is correct</td>\n",
       "      <td>The student's misconception likely stems from ...</td>\n",
       "      <td>1287 1073 1408 2439 1765 2408 1059 1975 1700 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                      ConstructName  \\\n",
       "0            1869_B  Use the order of operations to carry out calcu...   \n",
       "1            1869_C  Use the order of operations to carry out calcu...   \n",
       "2            1869_D  Use the order of operations to carry out calcu...   \n",
       "3            1870_A  Simplify an algebraic fraction by factorising ...   \n",
       "4            1870_B  Simplify an algebraic fraction by factorising ...   \n",
       "5            1870_C  Simplify an algebraic fraction by factorising ...   \n",
       "6            1871_A            Calculate the range from a list of data   \n",
       "7            1871_C            Calculate the range from a list of data   \n",
       "8            1871_D            Calculate the range from a list of data   \n",
       "\n",
       "                                         SubjectName  \\\n",
       "0                                             BIDMAS   \n",
       "1                                             BIDMAS   \n",
       "2                                             BIDMAS   \n",
       "3                    Simplifying Algebraic Fractions   \n",
       "4                    Simplifying Algebraic Fractions   \n",
       "5                    Simplifying Algebraic Fractions   \n",
       "6  Range and Interquartile Range from a List of Data   \n",
       "7  Range and Interquartile Range from a List of Data   \n",
       "8  Range and Interquartile Range from a List of Data   \n",
       "\n",
       "                                        QuestionText          CorrectAnswer  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "2  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "3  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "4  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "5  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "6  Tom and Katie are discussing the \\( 5 \\) plant...            Only\\nKatie   \n",
       "7  Tom and Katie are discussing the \\( 5 \\) plant...            Only\\nKatie   \n",
       "8  Tom and Katie are discussing the \\( 5 \\) plant...            Only\\nKatie   \n",
       "\n",
       "          IncorrectAnswer                                      Misconception  \\\n",
       "0  \\( 3 \\times 2+(4-5) \\)  The student's misconception likely stems from ...   \n",
       "1   \\( 3 \\times(2+4-5) \\)  The student's misconception likely stems from ...   \n",
       "2  Does not need brackets  The student's misconception likely stems from ...   \n",
       "3               \\( m+1 \\)  The student's misconception likely stems from ...   \n",
       "4               \\( m+2 \\)  The student's misconception likely stems from ...   \n",
       "5               \\( m-1 \\)  The student's misconception likely stems from ...   \n",
       "6               Only\\nTom  The student likely chose \"Only Tom\" because th...   \n",
       "7      Both Tom and Katie  The misconception that led the student to inco...   \n",
       "8      Neither is correct  The student's misconception likely stems from ...   \n",
       "\n",
       "                                             top_100  \n",
       "0  315 1345 2488 1392 2586 2306 1054 1005 2532 17...  \n",
       "1  2488 315 1345 1084 1392 2586 373 2532 77 969 2...  \n",
       "2  2532 706 77 1392 1507 1672 1005 2586 1345 871 ...  \n",
       "3  891 143 59 1540 2398 2078 167 885 2021 1610 36...  \n",
       "4  143 891 885 2078 2398 1540 59 265 715 1610 159...  \n",
       "5  891 143 1610 2398 885 59 2078 715 979 1540 126...  \n",
       "6  1287 1408 2439 2408 1073 1059 1923 1975 227 16...  \n",
       "7  1287 1073 1408 2408 2439 557 1923 1765 1338 17...  \n",
       "8  1287 1073 1408 2439 1765 2408 1059 1975 1700 1...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_top_100 = []\n",
    "for i in range(len(top_100)):\n",
    "    str_top_100.append(\" \".join(top_100[i].astype(\"str\")))\n",
    "df[\"top_100\"] = str_top_100\n",
    "df.to_csv(\"top_100_df.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "453ca04d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T10:05:15.502931Z",
     "iopub.status.busy": "2024-11-29T10:05:15.502649Z",
     "iopub.status.idle": "2024-11-29T10:05:17.202494Z",
     "shell.execute_reply": "2024-11-29T10:05:17.201259Z"
    },
    "papermill": {
     "duration": 1.723353,
     "end_time": "2024-11-29T10:05:17.205251",
     "exception": false,
     "start_time": "2024-11-29T10:05:15.481898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 29 10:05:17 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   75C    P0             44W /   70W |     151MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   35C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ddb660",
   "metadata": {
    "papermill": {
     "duration": 0.019567,
     "end_time": "2024-11-29T10:05:17.245558",
     "exception": false,
     "start_time": "2024-11-29T10:05:17.225991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Retrieve Final Answer Using LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98000725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T10:05:17.287068Z",
     "iopub.status.busy": "2024-11-29T10:05:17.286411Z",
     "iopub.status.idle": "2024-11-29T10:05:17.295366Z",
     "shell.execute_reply": "2024-11-29T10:05:17.294498Z"
    },
    "papermill": {
     "duration": 0.031759,
     "end_time": "2024-11-29T10:05:17.297192",
     "exception": false,
     "start_time": "2024-11-29T10:05:17.265433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vllm_rerank.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vllm_rerank.py\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from vllm import LLM, SamplingParams\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, df, misconception_df):\n",
    "        self.data = df\n",
    "        self.misconception_df = misconception_df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        constructName = self.data.loc[idx, 'ConstructName']\n",
    "        subjectName = self.data.loc[idx, \"SubjectName\"]\n",
    "        question = self.data.loc[idx, \"QuestionText\"]\n",
    "        correct_answer = self.data.loc[idx, \"CorrectAnswer\"]\n",
    "        wrong_Answer = self.data.loc[idx, \"IncorrectAnswer\"]        \n",
    "        retrival = \"\\n\".join([f'{i + 1}. {self.misconception_df.loc[int(misconception_id), \"MisconceptionName\"]}' for i, misconception_id in enumerate(self.data.loc[idx, \"top_100\"].split(\" \"))])\n",
    "    \n",
    "        prompt = f\"\"\"Here is a question about {constructName}({subjectName}).\n",
    "Question: {question}\n",
    "Correct Answer: {correct_answer}\n",
    "Incorrect Answer: {wrong_Answer} \n",
    "Answer concisely what misconception it is to lead to getting the Incorrect Answer. Pick the correct misconception number from the below:\n",
    "                \n",
    "{retrival}\n",
    "\"\"\"\n",
    "        message = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt.strip()}\n",
    "        ]\n",
    "        \n",
    "        return message, correct_answer, str(self.data.loc[idx, \"QuestionId_Answer\"])\n",
    "    \n",
    "def collate_batch(batch):\n",
    "    data, labels, question_ids = zip(*batch)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        data,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text, labels, question_ids\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"top_100_df.csv\")\n",
    "    misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "    train_dataset = MathDataset(df, misconception_df)\n",
    "    train_pbar = tqdm(DataLoader(train_dataset, batch_size=32, collate_fn=collate_batch))\n",
    "    \n",
    "    model_name = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "    device = \"cuda\" # the device to load the model onto\n",
    "    model = LLM(model_name,\n",
    "                quantization=\"awq\",\n",
    "                gpu_memory_utilization=1,\n",
    "                tensor_parallel_size=2,\n",
    "                trust_remote_code=True,\n",
    "                dtype=\"half\",\n",
    "                max_model_len=4000)\n",
    "    tokenizer = model.get_tokenizer()\n",
    "    \n",
    "    llm_response = []\n",
    "    for model_inputs, _, _ in train_pbar:\n",
    "        outputs = model.generate(\n",
    "            model_inputs,\n",
    "            SamplingParams(\n",
    "                n=1,\n",
    "                temperature=0.,\n",
    "                seed=111,\n",
    "                max_tokens=1,\n",
    "                logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'])]\n",
    "            ),\n",
    "            use_tqdm=False\n",
    "        )\n",
    "\n",
    "        \n",
    "        for i in range(len(outputs)):\n",
    "            llm_response.append(outputs[i].outputs[0].text)\n",
    "\n",
    "    with open(\"llm_rerank.json\", \"w\") as fp:\n",
    "        json.dump(llm_response, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd4e6b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T10:05:17.337986Z",
     "iopub.status.busy": "2024-11-29T10:05:17.337704Z",
     "iopub.status.idle": "2024-11-29T10:08:26.512696Z",
     "shell.execute_reply": "2024-11-29T10:08:26.511611Z"
    },
    "papermill": {
     "duration": 189.199669,
     "end_time": "2024-11-29T10:08:26.516615",
     "exception": false,
     "start_time": "2024-11-29T10:05:17.316946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]WARNING 11-29 10:05:22 config.py:246] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 11-29 10:05:22 config.py:715] Defaulting to use mp for distributed inference\r\n",
      "INFO 11-29 10:05:22 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, use_v2_block_manager=False, enable_prefix_caching=False)\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "INFO 11-29 10:05:23 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "INFO 11-29 10:05:23 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:05:23 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:05:23 selector.py:54] Using XFormers backend.\r\n",
      "INFO 11-29 10:05:23 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:05:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:05:25 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "INFO 11-29 10:05:25 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "INFO 11-29 10:05:25 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:05:25 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 11-29 10:05:25 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:05:25 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "WARNING 11-29 10:05:25 custom_all_reduce.py:127] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m WARNING 11-29 10:05:25 custom_all_reduce.py:127] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 11-29 10:05:25 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7d46900b3940>, local_subscribe_port=46719, local_sync_port=40107, remote_subscribe_port=None, remote_sync_port=None)\r\n",
      "INFO 11-29 10:05:25 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:05:25 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "INFO 11-29 10:05:25 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 11-29 10:05:25 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:05:25 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:05:25 selector.py:54] Using XFormers backend.\r\n",
      "\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "\u001b[A\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:11<00:47, 11.92s/it]\r\n",
      "\u001b[A\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:22<00:33, 11.13s/it]\r\n",
      "\u001b[A\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:31<00:20, 10.06s/it]\r\n",
      "\u001b[A\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:39<00:09,  9.29s/it]\r\n",
      "\u001b[A\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:46<00:00,  8.63s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:46<00:00,  9.37s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:06:12 model_runner.py:692] Loading model weights took 9.0933 GB\r\n",
      "INFO 11-29 10:06:12 model_runner.py:692] Loading model weights took 9.0933 GB\r\n",
      "INFO 11-29 10:06:21 distributed_gpu_executor.py:56] # GPU blocks: 1613, # CPU blocks: 2048\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:06:25 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:06:25 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "INFO 11-29 10:06:25 model_runner.py:980] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n",
      "INFO 11-29 10:06:25 model_runner.py:984] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "INFO 11-29 10:07:41 model_runner.py:1181] Graph capturing finished in 76 secs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=246)\u001b[0;0m INFO 11-29 10:07:41 model_runner.py:1181] Graph capturing finished in 76 secs.\r\n",
      "100%|████████████████████████████████████████████| 1/1 [02:57<00:00, 177.96s/it]\r\n",
      "INFO 11-29 10:08:22 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\n"
     ]
    }
   ],
   "source": [
    "!python vllm_rerank.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae557460",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T10:08:26.562651Z",
     "iopub.status.busy": "2024-11-29T10:08:26.562287Z",
     "iopub.status.idle": "2024-11-29T10:08:26.576988Z",
     "shell.execute_reply": "2024-11-29T10:08:26.576335Z"
    },
    "papermill": {
     "duration": 0.039845,
     "end_time": "2024-11-29T10:08:26.578696",
     "exception": false,
     "start_time": "2024-11-29T10:08:26.538851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"llm_rerank.json\", \"r\") as fp:\n",
    "    llm_answer = json.load(fp)\n",
    "\n",
    "submission = []\n",
    "for idx, row in df.iterrows():\n",
    "    tmp = top_100[idx].astype(str)\n",
    "    final_answer = [tmp[int(llm_answer[idx]) - 1]]\n",
    "    for i in range(100):\n",
    "        if tmp[i] != tmp[int(llm_answer[idx]) - 1]:\n",
    "            final_answer.append(tmp[i])\n",
    "            \n",
    "    submission.append({\"QuestionId_Answer\": row[\"QuestionId_Answer\"], \"MisconceptionId\": \" \".join(final_answer)})\n",
    "pd.DataFrame(submission).to_csv(\"submission.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 9268800,
     "datasetId": 5251603,
     "sourceId": 9094368,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10315213,
     "datasetId": 4581967,
     "sourceId": 10042054,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 9058412,
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10210204,
     "datasetId": 6117312,
     "sourceId": 9948011,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 9919403,
     "datasetId": 5920031,
     "sourceId": 9688062,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 9971368,
     "datasetId": 5957531,
     "sourceId": 9734430,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 8344064,
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 9658561,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 9925355,
     "modelInstanceId": 121393,
     "sourceId": 143283,
     "sourceType": "modelInstanceVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1485.428055,
   "end_time": "2024-11-29T10:08:30.400337",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-29T09:43:44.972282",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0bbb43c9f3d14b5c89a84a4e07913301": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f8679a822d80442fa66804febf8bf884",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bacf2199d4e74869ae15a79e48134764",
       "value": 3.0
      }
     },
     "1f2dfcb3a0494002a1aec06a75a263a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f5a1d687c3104d06a2469d94725d07ca",
        "IPY_MODEL_0bbb43c9f3d14b5c89a84a4e07913301",
        "IPY_MODEL_e2d996c91c6d48c3ada42f05526e4c5d"
       ],
       "layout": "IPY_MODEL_773314d4605e426baa3f498c9757b545"
      }
     },
     "45bd703e6aa945178b05c7a0bbb14471": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b7ab55e0098417089224fd4f0236559": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "773314d4605e426baa3f498c9757b545": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "87f2f2812f71474e8163b06bd6ebe832": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d6d59f3c0a64651b9a931cc536d996a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bacf2199d4e74869ae15a79e48134764": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e2d996c91c6d48c3ada42f05526e4c5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_87f2f2812f71474e8163b06bd6ebe832",
       "placeholder": "​",
       "style": "IPY_MODEL_5b7ab55e0098417089224fd4f0236559",
       "value": " 3/3 [01:11&lt;00:00, 23.12s/it]"
      }
     },
     "f5a1d687c3104d06a2469d94725d07ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_45bd703e6aa945178b05c7a0bbb14471",
       "placeholder": "​",
       "style": "IPY_MODEL_8d6d59f3c0a64651b9a931cc536d996a",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "f8679a822d80442fa66804febf8bf884": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
